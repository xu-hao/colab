{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xu-hao/colab/blob/master/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LfuPY58huRJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on https://github.com/Zackory/Keras-MNIST-GAN/blob/master/mnist_gan.py"
      ]
    },
    {
      "metadata": {
        "id": "0pqFcO_up0w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir images\n",
        "!mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jPOUbgPlxc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "563c039c-c5a8-4af0-e896-18416d231040"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Convolution2D, UpSampling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras import initializers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGKrjrP8mirn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# Deterministic output.\n",
        "# Tired of seeing the same results every time? Remove the line below.\n",
        "np.random.seed(1000)\n",
        "\n",
        "# The results are a little better when the dimensionality of the random vector is only 10.\n",
        "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
        "randomDim = 100\n",
        "\n",
        "# Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "X_train = X_train.reshape(X_train.shape[0], 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xOGO-jcuqc8G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "generator_input = Input(shape=(randomDim,))\n",
        "x = Dense(256, kernel_initializer=initializers.RandomNormal(stddev=0.02))(generator_input)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dense(512)(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dense(1024)(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "generator_output = Dense(784, activation='tanh')(x)\n",
        "generator = Model(inputs=generator_input, outputs=generator_output)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\n",
        "discriminator_input = Input(shape=(784,))\n",
        "x = Dense(1024, kernel_initializer=initializers.RandomNormal(stddev=0.02))(discriminator_input)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(512)(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256)(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "discriminator_output = Dense(1, activation='sigmoid')(x)\n",
        "discriminator = Model(inputs=discriminator_input, outputs=discriminator_output)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\n",
        "# Combined network\n",
        "discriminator.trainable = False\n",
        "ganInput = Input(shape=(randomDim,))\n",
        "x = generator(ganInput)\n",
        "ganOutput = discriminator(x)\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itWOsZfbm2Qi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the loss from each batch\n",
        "def plotLoss(epoch):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(dLosses, label='Discriminitive loss')\n",
        "    plt.plot(gLosses, label='Generative loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('images/gan_loss_epoch_%d.png' % epoch)\n",
        "\n",
        "# Create a wall of generated MNIST images\n",
        "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generatedImages.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)\n",
        "\n",
        "# Save the generator and discriminator networks (and weights) for later use\n",
        "def saveModels(epoch):\n",
        "    generator.save('models/gan_generator_epoch_%d.h5' % epoch)\n",
        "    discriminator.save('models/gan_discriminator_epoch_%d.h5' % epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0bceSPsoM69",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batchSize=128):\n",
        "    batchCount = X_train.shape[0] / batchSize\n",
        "    print ('Epochs:', epochs)\n",
        "    print ('Batch size:', batchSize)\n",
        "    print ('Batches per epoch:', batchCount)\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in tqdm(range(int(batchCount))):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
        "\n",
        "            # Generate fake MNIST images\n",
        "            generatedImages = generator.predict(noise)\n",
        "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
        "            X = np.concatenate([imageBatch, generatedImages])\n",
        "\n",
        "            # Labels for generated and real data\n",
        "            yDis = np.zeros(2*batchSize)\n",
        "            # One-sided label smoothing\n",
        "            yDis[:batchSize] = 0.9\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            dloss = discriminator.train_on_batch(X, yDis)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            yGen = np.ones(batchSize)\n",
        "            discriminator.trainable = False\n",
        "            gloss = gan.train_on_batch(noise, yGen)\n",
        "\n",
        "        # Store loss of most recent batch from this epoch\n",
        "        dLosses.append(dloss)\n",
        "        gLosses.append(gloss)\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "            plotGeneratedImages(e)\n",
        "            saveModels(e)\n",
        "\n",
        "    # Plot losses from every epoch\n",
        "    plotLoss(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zmt6k_ffr4KW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dLosses = []\n",
        "gLosses = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GcUuTu01ocpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "426c018e-0e1b-4583-9778-a5b668f396ee"
      },
      "cell_type": "code",
      "source": [
        "train(200, 128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/468 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epochs: 200\n",
            "Batch size: 128\n",
            "Batches per epoch: 468.75\n",
            "--------------- Epoch 1 ---------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:14<00:00, 32.48it/s]\n",
            "  1%|          | 5/468 [00:00<00:11, 40.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 2 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 40.86it/s]\n",
            "  1%|          | 5/468 [00:00<00:11, 40.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 3 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 41.34it/s]\n",
            "  1%|          | 5/468 [00:00<00:11, 40.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 4 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 41.79it/s]\n",
            "  1%|          | 5/468 [00:00<00:11, 40.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 5 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 41.46it/s]\n",
            "  1%|          | 4/468 [00:00<00:11, 39.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 6 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 41.42it/s]\n",
            "  1%|          | 4/468 [00:00<00:12, 37.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 7 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:11<00:00, 40.84it/s]\n",
            "  1%|          | 5/468 [00:00<00:11, 42.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 8 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 177/468 [00:04<00:07, 40.62it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5s9vAsLFr97m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}